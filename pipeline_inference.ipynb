{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b7128f",
   "metadata": {},
   "source": [
    "# **Orchestrating Machine Learning Workflow with Amazon SageMaker Pipelines**\n",
    "## Only the Inference job - Using Bring your own code method and using prevously trained model\n",
    "### Usecase- Customer Churn Prediction\n",
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf925cb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Data](#1.-Data)\n",
    "\t* [1.1 Import dataset](#1.1-Import-dataset)\n",
    "\t* [1.2 Data description](#1.2-Data-description)\n",
    "* [2. Sagemaker Pipeline](#2.-Sagemaker-Pipeline)\n",
    "\t* [2.1 Architecture](#2.1-Architecture)\n",
    "\t* [2.2 Install predefined Sagemaker libraries](#2.2-Install-predefined-Sagemaker-libraries)\n",
    "\t* [2.3 Import other define functions](#2.3-Import-other-define-functions)\n",
    "\t* [2.4 Convert your model and upload it to s3](#2.4-Convert-your-model-and-upload-it-to-s3)\n",
    "\t* [2.5 Define Preprocessing Stage](#2.5-Define-Preprocessing-Stage)\n",
    "\t* [2.6 Define Inference stage](#2.6-Define-Inference-stage)\n",
    "\t* [2.7 Define Postprocess steps](#2.7-Define-Postprocess-steps)\n",
    "\t* [2.8 Define required parameters for get pipeline](#2.8-Define-required-parameters-for-get-pipeline)\n",
    "\t* [2.9 Othere details of the pipeline](#2.8-Othere-details-of-the-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3ae74",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6dda3a",
   "metadata": {},
   "source": [
    "## 1.1 Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "54fe1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/data_inference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca6ef339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2674-MIAHT</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>80.30</td>\n",
       "      <td>324.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4086-WITJG</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>71</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>19.70</td>\n",
       "      <td>1301.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7096-ZNBZI</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>26.45</td>\n",
       "      <td>1914.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9885-AIBVB</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>29</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>85.80</td>\n",
       "      <td>2440.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8647-SDTWQ</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>57</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>74.30</td>\n",
       "      <td>4018.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  2674-MIAHT  Female              0      No         No       4          Yes   \n",
       "1  4086-WITJG    Male              0     Yes        Yes      71          Yes   \n",
       "2  7096-ZNBZI  Female              0     Yes         No      72          Yes   \n",
       "3  9885-AIBVB    Male              0     Yes         No      29          Yes   \n",
       "4  8647-SDTWQ    Male              0     Yes        Yes      57          Yes   \n",
       "\n",
       "  MultipleLines InternetService       OnlineSecurity         OnlineBackup  \\\n",
       "0           Yes     Fiber optic                   No                  Yes   \n",
       "1            No              No  No internet service  No internet service   \n",
       "2           Yes              No  No internet service  No internet service   \n",
       "3           Yes     Fiber optic                   No                  Yes   \n",
       "4           Yes     Fiber optic                   No                   No   \n",
       "\n",
       "      DeviceProtection          TechSupport          StreamingTV  \\\n",
       "0                   No                   No                   No   \n",
       "1  No internet service  No internet service  No internet service   \n",
       "2  No internet service  No internet service  No internet service   \n",
       "3                  Yes                   No                   No   \n",
       "4                   No                   No                   No   \n",
       "\n",
       "       StreamingMovies        Contract PaperlessBilling  \\\n",
       "0                   No  Month-to-month              Yes   \n",
       "1  No internet service        Two year               No   \n",
       "2  No internet service        Two year               No   \n",
       "3                   No  Month-to-month              Yes   \n",
       "4                   No  Month-to-month              Yes   \n",
       "\n",
       "             PaymentMethod  MonthlyCharges TotalCharges  \n",
       "0             Mailed check           80.30        324.2  \n",
       "1  Credit card (automatic)           19.70       1301.1  \n",
       "2  Credit card (automatic)           26.45       1914.5  \n",
       "3         Electronic check           85.80      2440.25  \n",
       "4         Electronic check           74.30      4018.35  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09fdc3d",
   "metadata": {},
   "source": [
    "## 1.2 Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f52894",
   "metadata": {},
   "source": [
    "Each row represents a customer, each column contains customer’s attributes described on the column Metadata. The raw data contains 7043 rows (customers) and 21 columns (features). The “Churn” column is our target. \n",
    "\n",
    "- **`CustomerID:`** Customer ID \n",
    "\n",
    "- **`Gender:`** Whether the customer is a male or a female \n",
    "\n",
    "- **`SeniorCitizen:`** Whether the customer is a senior citizen or not (1, 0) \n",
    "\n",
    "- **`Partner:`** Whether the customer has a partner or not (Yes, No) \n",
    "\n",
    "- **`Dependents:`** Whether the customer has dependents or not (Yes, No) \n",
    "\n",
    "- **`tenure:`** Number of months the customer has stayed with the company \n",
    "\n",
    "- **`PhoneService:`** Whether the customer has a phone service or not (Yes, No) \n",
    "\n",
    "- **`MultipleLines:`** Whether the customer has multiple lines or not (Yes, No, No phone service) \n",
    "\n",
    "- **`InternetService:`** Customer’s internet service provider (DSL, Fiber optic, No) \n",
    "\n",
    "- **`OnlineSecurity:`** Whether the customer has online security or not (Yes, No, No internet service) \n",
    "\n",
    "- **`OnlineBackup:`** Whether the customer has online backup or not (Yes, No, No internet service)\n",
    "\n",
    "- **`DeviceProtection:`** Whether the customer has device protection or not (Yes, No, No internet service)\n",
    "\n",
    "- **`TechSupport:`** Whether the customer has tech support or not (Yes, No, No internet service)\n",
    "\n",
    "- **`StreamingTV:`** Whether the customer has streaming TV or not (Yes, No, No internet service)\n",
    "\n",
    "- **`StreamingMovies:`** Whether the customer has streaming movies or not (Yes, No, No internet service)\n",
    "\n",
    "- **`Contract:`** The contract term of the customer (Month-to-month, One year, Two year)\n",
    "\n",
    "- **`PaperlessBilling:`** Whether the customer has paperless billing or not (Yes, No)\n",
    "\n",
    "- **`PaymentMethod:`** The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n",
    "\n",
    "- **`MonthlyCharges:`** The amount charged to the customer monthly\n",
    "\n",
    "- **`TotalCharges:`** The total amount charged to the customer\n",
    "\n",
    "- **`Churn:`** Whether the customer churned or not (Yes or No) ---> dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa271d3c",
   "metadata": {},
   "source": [
    "# 2. Sagemaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b251b40",
   "metadata": {},
   "source": [
    "SageMaker Pipelines supports the following activities, which are demonstrated in this notebook:\n",
    "\n",
    "- Pipelines - A DAG of steps and conditions to orchestrate SageMaker jobs and resource creation.\n",
    "- Processing job steps - A simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation.\n",
    "- Training job steps - An iterative process that teaches a model to make predictions by presenting examples from a training dataset.\n",
    "- Register model steps - A step that creates a model package resource in the Model Registry that can be used to create deployable models in Amazon SageMaker.\n",
    "- Create model steps - A step that creates a model for use in transform steps or later publication as an endpoint.\n",
    "- Transform job steps - A batch transform to preprocess datasets to remove noise or bias that interferes with training or inference from a dataset, get inferences from large datasets, and run inference when a persistent endpoint is not needed.\n",
    "- Post processing - (Optional) A step that filtering the final predicted output base : In here we don't include that step into our pipeline\n",
    "- Parametrized Pipeline executions - Enables variation in pipeline executions according to specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dbd18",
   "metadata": {},
   "source": [
    "### 2.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef3fd0",
   "metadata": {},
   "source": [
    "This **inference** pipeline contains preprocess, inference, create model and post-process steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f8f62",
   "metadata": {},
   "source": [
    "![architecture](images/final_pipeline.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d833ef1",
   "metadata": {},
   "source": [
    "Lets beginning the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa013fe8",
   "metadata": {},
   "source": [
    "![workflow](images/pic10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e57fe6",
   "metadata": {},
   "source": [
    "## 2.2 Install predefined Sagemaker libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab98ad0",
   "metadata": {},
   "source": [
    "Initailly we have to install AWS predefined Sagemaker libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65dee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput,TransformInput,CreateModelInput\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThanOrEqualTo,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4200d",
   "metadata": {},
   "source": [
    "![workflow](images/pic11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63d5c2",
   "metadata": {},
   "source": [
    "## 2.3 Import other define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6799be",
   "metadata": {},
   "source": [
    "Next step is start session and in this process we are defining our <b> AWS region, sagemaker client, boto 3 session</b> and <b>default s3 bucket </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67559b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(region, default_bucket):\n",
    "    \"\"\"Gets the sagemaker session based on the region.\n",
    "    Args:\n",
    "        region: the aws region to start the session\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        `sagemaker.session.Session instance\n",
    "    \"\"\"\n",
    "\n",
    "    boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "    sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "    runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "    return sagemaker.session.Session(\n",
    "        boto_session=boto_session,\n",
    "        sagemaker_client=sagemaker_client,\n",
    "        sagemaker_runtime_client=runtime_client,\n",
    "        default_bucket=default_bucket,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b265d0",
   "metadata": {},
   "source": [
    "## 2.4 Convert your model and upload it to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb5cea",
   "metadata": {},
   "source": [
    "Next step is to convert your previouly trained model into <b>model.tar.format</b> because <b>Flask app</b> is looking this compressed format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f08f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile \n",
    "import os.path \n",
    "\n",
    "def make_tarfile(output_filename, source_dir): \n",
    "\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar: \n",
    "\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir)) \n",
    "\n",
    "make_tarfile(\"model.tar.gz\", \"temp_dict.pkl\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058d2ee",
   "metadata": {},
   "source": [
    "If you need to check whether the pickle file successfully compressed into model.tra.gz file, please use this below code to unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b71a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile \n",
    " \n",
    "\n",
    "#simple function to extract the train data \n",
    "#tar_file : the path to the .tar file \n",
    "#path : the path where it will be extracted \n",
    "def extract(tar_file, path): \n",
    "    opened_tar = tarfile.open(tar_file) \n",
    "\n",
    "    if tarfile.is_tarfile(tar_file): \n",
    "        opened_tar.extractall(path) \n",
    "    else: \n",
    "        print(\"The tar file you entered is not a tar file\")\n",
    "\n",
    "extract('model.tar.gz', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cdc42e",
   "metadata": {},
   "source": [
    "Then, we will export this model.tar.gz file into s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket = sess.default_bucket()\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941aa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "data = open('model.tar.gz','rb')\n",
    "s3.Bucket(f\"{default_bucket}\").put_object(Key='customer_churn/model/model.tar.gz', Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f889e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calling the leatest model\n",
    "def model_with_pipeline():\n",
    "    model_path = f\"s3://{default_bucket}/customer_churn/model/model.tar.gz\"\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568f9a0",
   "metadata": {},
   "source": [
    "![workflow](images/pic12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969777e",
   "metadata": {},
   "source": [
    "## 2.5 Define Preprocessing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67574b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f\"s3://{default_bucket}/customer_churn/customer_churn/inference_input/data_inference.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef720139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(input_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97550fa5",
   "metadata": {},
   "source": [
    "This is the script used in preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1838ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "warnings.filterwarnings(\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=\u001b[36mFutureWarning\u001b[39;49;00m)\n",
      "warnings.filterwarnings(\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=\u001b[36mDeprecationWarning\u001b[39;49;00m)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "warnings.simplefilter(action=\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mimport your necessary libraries in here\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MinMaxScaler\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menter your own functions in the bellow space\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mchange_format\u001b[39;49;00m(df):\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = pd.to_numeric(df.TotalCharges, errors=\u001b[33m'\u001b[39;49;00m\u001b[33mcoerce\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmissing_value\u001b[39;49;00m(df):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcount of missing values: (before treatment)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, df.isnull().sum())\n",
      "    \n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(df[\u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].mean())\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcount of missing values: (before treatment)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, df.isnull().sum())\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmissing values successfully replaced\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdata_manipulation\u001b[39;49;00m(df):\n",
      "    df = df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis = \u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcat_encoder\u001b[39;49;00m(df, variable_list):\n",
      "    dummy = pd.get_dummies(df[variable_list], drop_first = \u001b[34mTrue\u001b[39;49;00m)\n",
      "    df = pd.concat([df, dummy], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    df.drop(df[cat_var], axis = \u001b[34m1\u001b[39;49;00m, inplace = \u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mencoded successfully\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mscaling\u001b[39;49;00m(X):  \n",
      "    min_max=MinMaxScaler()\n",
      "    X=pd.DataFrame(min_max.fit_transform(X),columns=X.columns)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m X\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msuccessfully loaded our own defined functions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    input_data_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_inference.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mreading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_data_path))\n",
      "    df = pd.read_csv(input_data_path)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrename the columns in the dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    df.columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mgender\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mSeniorCitizen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mtenure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMonthlyCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \n",
      "    \u001b[37m#################################### Enter your own script in here ###########################################################################\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mdefining the list of categorical variables\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    cat_var = [\u001b[33m'\u001b[39;49;00m\u001b[33mgender\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcalling our own defined function\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    df = missing_value(change_format(df))\n",
      "    df = cat_encoder(df, cat_var)\n",
      "    \n",
      "    df1=df[\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    df=df.drop(columns=[\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    \n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mscaling the dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    df = scaling(df)\n",
      "    idx=\u001b[34m0\u001b[39;49;00m\n",
      "    df.insert(loc=idx, column=\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, value=df1)\n",
      "\n",
      "    \n",
      "    \u001b[37m#################################### End of the code #########################################################################################\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving the outputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    X_output_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_output.csv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving output to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(X_output_path))\n",
      "    pd.DataFrame(df).to_csv(X_output_path, header=\u001b[34mFalse\u001b[39;49;00m,index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msuccessfully completed the preprocessing job for inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize customer_churn_inference_preprocessing/preprocessing_without.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa9713",
   "metadata": {},
   "source": [
    "This is the code for run the preprocessing part. You can add more inputs or outputs according to your requirement. Please refer the guideline document for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b04a48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"cutomer-churn-prediction-inference-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"cutomer-churn-prediction-inference\",  # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data = f\"s3://{default_bucket}/customer_churn/inference_input/data_inference.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/inference/{date_folder}/output1/\"\n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    \n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_inference_preprocessing/preprocessing_without.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name+env,\n",
    "        parameters=[\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076c7cb",
   "metadata": {},
   "source": [
    "![workflow](images/pic13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4bcb2",
   "metadata": {},
   "source": [
    "## 2.6 Define Inference stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328efa5",
   "metadata": {},
   "source": [
    "We used this script for inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "325ca04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# This is the file that implements a flask server to do inferences. It's the file that you will modify to\u001b[39;49;00m\n",
      "\u001b[37m# implement the scoring for your own algorithm.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msignal\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtraceback\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mflask\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[37m#import statsmodels.api as sm\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#Temporarly remove this path and giving the hardcode path\u001b[39;49;00m\n",
      "prefix = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "model_path = os.path.join(prefix, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[37m# A singleton for holding the model. This simply loads the model and holds it.\u001b[39;49;00m\n",
      "\u001b[37m# It has a predict function that does a prediction based on the model and the input data.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mCustomerChurn\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    model = \u001b[34mNone\u001b[39;49;00m  \u001b[37m# Where we keep the model when it's loaded\u001b[39;49;00m\n",
      "    complete_model = \u001b[34mNone\u001b[39;49;00m\n",
      "    model_flag = \u001b[34mFalse\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# @classmethod\u001b[39;49;00m\n",
      "    \u001b[37m# def get_model(cls):\u001b[39;49;00m\n",
      "    \u001b[37m#     \"\"\"Get the model object for this instance, loading it if it's not already loaded.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[37m#     if cls.complete_model == None:\u001b[39;49;00m\n",
      "    \u001b[37m#         with open(os.path.join(model_path, 'model.pickle'), 'rb') as inp:\u001b[39;49;00m\n",
      "    \u001b[37m#             cls.complete_model = pickle.load(inp)\u001b[39;49;00m\n",
      "    \u001b[37m#     return cls.complete_model\u001b[39;49;00m\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    print(\"loading the stage config data from s3\")\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    @classmethod\u001b[39;49;00m\n",
      "\u001b[33m    def getJsonData(cls, bucket_name, key_name):\u001b[39;49;00m\n",
      "\u001b[33m        print(\"[LOG]\", bucket_name,'---------')\u001b[39;49;00m\n",
      "\u001b[33m        print(\"[LOG]\", key_name,'--------------')\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        s3_client = boto3.client('s3')\u001b[39;49;00m\n",
      "\u001b[33m        csv_obj = s3_client.get_object(Bucket=bucket_name, Key=key_name)\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        body = csv_obj['Body']\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        json_string = body.read().decode('utf-8')\u001b[39;49;00m\n",
      "\u001b[33m        json_content = json.loads(json_string)\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        return cls.json_content\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_model\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Get the model object for this instance, loading it if it's not already loaded.\"\"\"\u001b[39;49;00m\n",
      "\u001b[37m#        model_path = \"s3://dlk-cloud-tier-9-training-ml-dev/prepaid-churn/2021-10-13/pipelines-xb88ksi5tynp-PrepaidChurn-trainin-YhUdrqjuke/output/\"\u001b[39;49;00m\n",
      "\u001b[37m#        if cls.model == None:\u001b[39;49;00m\n",
      "\u001b[37m#            with open(os.path.join(model_path, \"model.tar.gz\"), \"rb\") as inp:\u001b[39;49;00m\n",
      "\u001b[37m#                cls.model = pickle.load(inp)\u001b[39;49;00m\n",
      "\u001b[37m#        return cls.model\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mcls\u001b[39;49;00m.model == \u001b[34mNone\u001b[39;49;00m:\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mtemp_dict.pkl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m inp:\n",
      "                \u001b[36mcls\u001b[39;49;00m.model = pickle.load(inp)\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mcls\u001b[39;49;00m.model\n",
      "    \n",
      "\u001b[37m#        if not cls.model_flag:\u001b[39;49;00m\n",
      "\u001b[37m#            with open(os.path.join(model_path, 'temp_dict.pkl'), 'rb') as inp:\u001b[39;49;00m\n",
      "\u001b[37m#                cls.complete_model = pickle.load(inp)\u001b[39;49;00m\n",
      "\u001b[37m#                cls.model_flag = True\u001b[39;49;00m\n",
      "\u001b[37m#        return cls.model_flag\u001b[39;49;00m\n",
      "   \n",
      "\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_proba\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m, df_prediction):\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mPrint Entered...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[37m#print(input.head())\u001b[39;49;00m\n",
      "        \u001b[37m#print(input.dtypes)\u001b[39;49;00m\n",
      "        \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m        print(\"enter your project name and enviornment here\")\u001b[39;49;00m\n",
      "\u001b[33m        project_name = \"aws_workshop\"\u001b[39;49;00m\n",
      "\u001b[33m        env = 'dev'\u001b[39;49;00m\n",
      "\u001b[33m        '''\u001b[39;49;00m\n",
      "        \n",
      "        columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mSeniorCitizen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtenure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMonthlyCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mgender_Male\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_No phone service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_Fiber optic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_No\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mContract_One year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract_Two year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Credit card (automatic)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Electronic check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Mailed check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        print(\"setting the parameters for getJsonData function\")\u001b[39;49;00m\n",
      "\u001b[33m        config_bucket = f\"dlk-cloud-tier-8-code-ml-{env}\"\u001b[39;49;00m\n",
      "\u001b[33m        config_s3_prefix_stage = f'config_files/stage_config/{project_name}/stage_config.json'\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        print(\"calling the getJsonData function\")\u001b[39;49;00m\n",
      "\u001b[33m        config = cls.getJsonData(config_bucket,config_s3_prefix_stage)\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        print(\"json script loaded successfully\")\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrename the columns of the dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "        df_prediction.columns = columns\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(df_prediction.shape)\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_prediction.columns)\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_prediction.head())\n",
      "        \n",
      "        feature_cols = [\u001b[33m'\u001b[39;49;00m\u001b[33mSeniorCitizen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtenure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMonthlyCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mgender_Male\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_No phone service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_Fiber optic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_No\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mContract_One year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract_Two year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Credit card (automatic)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Electronic check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Mailed check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mAssign the dataframe into X variable\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        X = df_prediction[feature_cols]\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of rows\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, X.shape[\u001b[34m0\u001b[39;49;00m])\n",
      "                \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel predictions starting\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mAssign the model into model variable\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        model = \u001b[36mcls\u001b[39;49;00m.get_model()\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStart the prediction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        pred_y = model.predict_proba(X)[:,\u001b[34m1\u001b[39;49;00m]\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m pred_y\n",
      "\n",
      "\n",
      "\u001b[37m# The flask app for serving predictions\u001b[39;49;00m\n",
      "app = flask.Flask(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/ping\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mGET\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mping\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Determine if the container is working and healthy. In this sample container, we declare\u001b[39;49;00m\n",
      "\u001b[33m    it healthy if we can load the model successfully.\"\"\"\u001b[39;49;00m\n",
      "    health = CustomerChurn.get_model()  \u001b[37m# You can insert a health check here\u001b[39;49;00m\n",
      "\n",
      "    status = \u001b[34m200\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m health \u001b[34melse\u001b[39;49;00m \u001b[34m404\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, status=status, mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m\"\u001b[39;49;00m\u001b[33m/invocations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, methods=[\u001b[33m\"\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransformation\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\u001b[39;49;00m\n",
      "\u001b[33m    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\u001b[39;49;00m\n",
      "\u001b[33m    just means one prediction per line, since there's a single column.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    data = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Convert from CSV to pandas\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m flask.request.content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        data = flask.request.data.decode(\u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        s = io.StringIO(data)\n",
      "        data = pd.read_csv(s, header=\u001b[34mNone\u001b[39;49;00m)\n",
      "        \u001b[37m##data = pd.read_csv(s, header=None, error_bad_lines=False)\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m flask.Response(\n",
      "            response=\u001b[33m\"\u001b[39;49;00m\u001b[33mThis predictor only supports CSV data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, status=\u001b[34m415\u001b[39;49;00m, mimetype=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mInvoked with \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m records\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data.shape[\u001b[34m0\u001b[39;49;00m]))\n",
      "\n",
      "    \u001b[37m# Do the prediction\u001b[39;49;00m\n",
      "    predictions = CustomerChurn.predict_proba(data)\n",
      "\n",
      "    \u001b[37m# Convert from numpy back to CSV\u001b[39;49;00m\n",
      "    out = io.StringIO()\n",
      "    pd.DataFrame({\u001b[33m\"\u001b[39;49;00m\u001b[33mresults\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: predictions}).to_csv(out, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    result = out.getvalue()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=result, status=\u001b[34m200\u001b[39;49;00m, mimetype=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize customer_churn_inference/model/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bc51e",
   "metadata": {},
   "source": [
    "Please execute bellow code to create the pipeline. This pipeline contains  **preprocessing and inference** steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"cutomer-churn-prediction-inference-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"cutomer-churn-prediction-inference\",  # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data = f\"s3://{default_bucket}/customer_churn/inference_input/data_inference.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/inference/{date_folder}/output1/\"\n",
    "    \n",
    "    postprocessed_output1= f\"s3://{default_bucket}/customer_churn/postprocess/{date_folder}/output1/\"   \n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    \n",
    "    \n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"awsworkshop_team2_inf_preprocessing/preprocessing_without.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ############## Inference Job ##############################################\n",
    "    \n",
    "    ecr_repository = \"customer-churn-prediction-inference-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    create_model_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "    \n",
    "    image = create_model_repository_uri\n",
    "    \n",
    "    #Batch Transform job\n",
    "    \n",
    "    #model_name = step_create_model.properties.ModelName\n",
    "    job_name = \"cutomerchurn-model\"\n",
    "    instance_type = \"ml.m5.12xlarge\"\n",
    "    step_create_model = CreateModelStep(\n",
    "        name=f\"{base_job_prefix}-createmodel\",\n",
    "        model=Model(image, \n",
    "                    model_data=model_with_pipeline(),\n",
    "                    role = role,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    name = job_name,\n",
    "                    vpc_config = {'Subnets':subnets.split(':'),\n",
    "                                    'SecurityGroupIds': security_group_ids.split(':')}),\n",
    "        inputs=CreateModelInput(instance_type=instance_type,\n",
    "                               #accelerator_type=\"ml.eia1.medium\"\n",
    "                               )\n",
    "    )\n",
    "    \n",
    "    model_name = step_create_model.properties.ModelName\n",
    "    content_type =\"text/csv\"\n",
    "    \n",
    "    transformer = Transformer(model_name=model_name,\n",
    "                              instance_count=1,\n",
    "                              strategy='SingleRecord',\n",
    "                              #max_payload=15, \n",
    "                              assemble_with=\"Line\",\n",
    "                              instance_type=instance_type,\n",
    "                              output_path=f\"s3://{default_bucket}/mobile_price_pred/inference/{date_folder}/predictions/\",\n",
    "                              #tags = generic_tags + [{'Key': 'JobName', 'Value': 'Inference'}]\n",
    "                )\n",
    "    \n",
    "    step_transform = TransformStep(\n",
    "        name=f\"{base_job_prefix}-batchtransform\",\n",
    "        transformer=transformer,\n",
    "        inputs=TransformInput(data = step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri,\n",
    "                             split_type=\"Line\",\n",
    "                               compression_type = 'Gzip', \n",
    "                              content_type=content_type)\n",
    "    )\n",
    "\n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name+env,\n",
    "        parameters=[\n",
    "            #model_path,\n",
    "            #model_approval_status\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "            step_create_model,\n",
    "            step_transform,\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fc91a",
   "metadata": {},
   "source": [
    "![workflow](images/pic14.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ee118",
   "metadata": {},
   "source": [
    "## 2.7 Define Postprocess steps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b5466",
   "metadata": {},
   "source": [
    "Please execute bellow code to create the pipeline.This pipeline contains **preprocessing,inference and postprocess** steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67f6d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(\n",
    "    region,\n",
    "    subnets,\n",
    "    security_group_ids,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnModelPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"cutomer-churn-prediction-inference-pipeline\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"cutomer-churn-prediction-inference\",  # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #data versioning control using date\n",
    "    srilanka_tz = pytz.timezone('Asia/Colombo')\n",
    "    s3 = boto3.client('s3')\n",
    "    date_folder = datetime.now(srilanka_tz).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #working with input data path\n",
    "    input_data = f\"s3://{default_bucket}/customer_churn/inference_input/data_inference.csv\"\n",
    "    \n",
    "    #working with output data path   \n",
    "    preprocessed_output1 = f\"s3://{default_bucket}/customer_churn/inference/{date_folder}/output1/\"\n",
    "    \n",
    "    postprocessed_output1= f\"s3://{default_bucket}/customer_churn/postprocess/{date_folder}/output1/\"   \n",
    "    \n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    \n",
    "    \n",
    "\n",
    "    ####### --------------------- PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "    ecr_repository = \"customer-churn-prediction-preprocessing-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "    script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_preprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-preprocessing\",\n",
    "        processor= script_processor, \n",
    "        code= \"customer_churn_inference_preprocessing/preprocessing_without.py\",\n",
    "        inputs= [ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"output1\", destination=preprocessed_output1, source=\"/opt/ml/processing/output1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ############## Inference Job ##############################################\n",
    "    \n",
    "    ecr_repository = \"customer-churn-prediction-inference-image\"\n",
    "    tag = \":latest\"\n",
    "    uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "    create_model_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "    \n",
    "    image = create_model_repository_uri\n",
    "    \n",
    "    #Batch Transform job\n",
    "    \n",
    "    #model_name = step_create_model.properties.ModelName\n",
    "    job_name = \"cutomerchurn-model\"\n",
    "    instance_type = \"ml.m5.12xlarge\"\n",
    "    step_create_model = CreateModelStep(\n",
    "        name=f\"{base_job_prefix}-createmodel\",\n",
    "        model=Model(image, \n",
    "                    model_data=model_with_pipeline(),\n",
    "                    role = role,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    name = job_name,\n",
    "                    vpc_config = {'Subnets':subnets.split(':'),\n",
    "                                    'SecurityGroupIds': security_group_ids.split(':')}),\n",
    "        inputs=CreateModelInput(instance_type=instance_type,\n",
    "                               #accelerator_type=\"ml.eia1.medium\"\n",
    "                               )\n",
    "    )\n",
    "    \n",
    "    model_name = step_create_model.properties.ModelName\n",
    "    content_type =\"text/csv\"\n",
    "    \n",
    "    transformer = Transformer(model_name=model_name,\n",
    "                              instance_count=1,\n",
    "                              strategy='SingleRecord',\n",
    "                              #max_payload=15, \n",
    "                              assemble_with=\"Line\",\n",
    "                              instance_type=instance_type,\n",
    "                              output_path=f\"s3://{default_bucket}/customer_churn/inference/{date_folder}/predictions/\",\n",
    "                              #tags = generic_tags + [{'Key': 'JobName', 'Value': 'Inference'}]\n",
    "                )\n",
    "    \n",
    "    step_transform = TransformStep(\n",
    "        name=f\"{base_job_prefix}-batchtransform\",\n",
    "        transformer=transformer,\n",
    "        inputs=TransformInput(data = step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri,\n",
    "                             split_type=\"Line\",\n",
    "                               compression_type = 'Gzip', \n",
    "                              content_type=content_type)\n",
    "    )\n",
    "    \n",
    "    ####### --------------------- Post-PREPROCESSING --------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "    script_postprocessor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = \"ml.m5.large\",\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )\n",
    "    \n",
    "    step_postpreprocess = ProcessingStep(\n",
    "        name= f\"{base_job_prefix}-postpreprocessing\",\n",
    "        processor= script_postprocessor, \n",
    "        code= \"customer_churn_inference_postprocessing/postprocessing_without.py\",\n",
    "        inputs= [ProcessingInput(source=step_transform.properties.TransformOutput.S3OutputPath, destination=\"/opt/ml/processing/input\",s3_data_type='S3Prefix'),\n",
    "                ProcessingInput(source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"output1\"].S3Output.S3Uri, destination=\"/opt/ml/processing/input1\"),\n",
    "\n",
    "               ],\n",
    "        outputs= [\n",
    "            ProcessingOutput(output_name=\"post_output1\", destination=postprocessed_output1, source=\"/opt/ml/processing/post_output1\"),\n",
    "        ]\n",
    "    )    \n",
    "    # ========================================= PIPELINE ORCHESTRATION ================================================\n",
    "    \n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            #model_path,\n",
    "            #model_approval_status\n",
    "        ],\n",
    "        steps=[\n",
    "            step_preprocess,\n",
    "            step_create_model,\n",
    "            step_transform,\n",
    "            step_postpreprocess\n",
    "              ],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458224c",
   "metadata": {},
   "source": [
    "![workflow](images/pic15.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba79acf",
   "metadata": {},
   "source": [
    "This is the script used for postprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "285f2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#import only the necessary libraries\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyarrow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparquet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpq\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "warnings.filterwarnings(\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=\u001b[36mFutureWarning\u001b[39;49;00m)\n",
      "warnings.filterwarnings(\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=\u001b[36mDeprecationWarning\u001b[39;49;00m)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mimport your necessary libraries in here\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m \n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    input_data_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_output.csv.gz.out\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    input_data_path1 = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_output.csv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mreading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_data_path))\n",
      "    \n",
      "    df1 = pd.read_csv(input_data_path,header=\u001b[34mNone\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mloading the preprocess dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    df= pd.read_csv(input_data_path1,header=\u001b[34mNone\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m################### Enter your own script in here #######################\u001b[39;49;00m\n",
      "      \n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrenaming the prediction output dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)      \n",
      "    df1.rename(columns={\u001b[34m0\u001b[39;49;00m:\u001b[33m'\u001b[39;49;00m\u001b[33mchurn_prop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}, inplace=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcolumns selecting\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mSeniorCitizen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtenure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMonthlyCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTotalCharges\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mgender_Male\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPartner_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDependents_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPhoneService_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_No phone service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mMultipleLines_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_Fiber optic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mInternetService_No\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineSecurity_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mOnlineBackup_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDeviceProtection_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mTechSupport_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingTV_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_No internet service\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mStreamingMovies_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mContract_One year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mContract_Two year\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaperlessBilling_Yes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Credit card (automatic)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "       \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Electronic check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mPaymentMethod_Mailed check\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrenaming the colums\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    df.columns = columns\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33madd churn probabilty variable to preprocess dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mChurn_probability\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=df1[\u001b[33m'\u001b[39;49;00m\u001b[33mchurn_prop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    data_cmp_df = df[[\u001b[33m'\u001b[39;49;00m\u001b[33mcustomerID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mChurn_probability\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33msorting the first five hundred customers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    data_cmp_df=data_cmp_df.sort_values(\u001b[33m\"\u001b[39;49;00m\u001b[33mChurn_probability\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ascending=\u001b[34mFalse\u001b[39;49;00m).head(\u001b[34m500\u001b[39;49;00m)\n",
      "          \n",
      "    \u001b[37m#################### End of the code ##############################   \u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving the dataframe\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saving outputs.(must be header = False)\u001b[39;49;00m\n",
      "    inf_features_output_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/post_output1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdata_final_output.csv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving training features to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(inf_features_output_path))\n",
      "    pd.DataFrame(data_cmp_df).to_csv(inf_features_output_path, compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msuccessfully completed the post-processing job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize customer_churn_inference_postprocessing/postprocessing_without.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff826a04",
   "metadata": {},
   "source": [
    "## 2.8 Define required parameters for get pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf07fd5",
   "metadata": {},
   "source": [
    "Define subnets and parameters for get_pipeline function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "424f4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets = 'Enter your subnet here'\n",
    "sg = 'Enter your security group here'\n",
    "role='Enter your IAM role here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc051175",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket=default_bucket\n",
    "pipeline_def = get_pipeline(region, \n",
    "                            dev_subnets, \n",
    "                            dev_sg, \n",
    "                            role,\n",
    "                            default_bucket, \n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0c440",
   "metadata": {},
   "source": [
    "![workflow](images/pic17.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b143e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_def.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50b84c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline_def.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961c141",
   "metadata": {},
   "source": [
    "## 2.9 Othere details of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bdb02c",
   "metadata": {},
   "source": [
    "#### To see the execution ID\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249d919",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea9664",
   "metadata": {},
   "source": [
    "Check whether the pipeline is up and running using **`Sagemaker Studio`**\n",
    "\n",
    "![workflow](images/sagemakerstudio.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0f3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
